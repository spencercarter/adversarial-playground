{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST-Adversary\n",
    "\n",
    "## Work In Progress\n",
    "\n",
    "Trying to replicate results from [this paper by Papernot et al.](https://arxiv.org/abs/1602.02697)\n",
    "\n",
    "___\n",
    "\n",
    "In lieu of using the ML APIs like the paper, I'm using a pretrained model provided by the [PyTorch Playground](https://github.com/aaron-xichen/pytorch-playground).\n",
    "\n",
    "My original hope was to have no knowledge of the architecture, but that was dashed when it printed to std. out... Still, the playground uses a totally DNN architecture, whereas my replicated model is CNN-based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building and initializing mnist parameters\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=784, out_features=256)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (fc2): Linear(in_features=256, out_features=256)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.2)\n",
      "  (out): Linear(in_features=256, out_features=10)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MNIST data loader with 1 workers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  pred  correct\n",
       "0    3     3     True\n",
       "1    4     4     True\n",
       "2    4     4     True\n",
       "3    7     7     True\n",
       "4    1     1     True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from utee import selector\n",
    "from PIL import Image, ImageOps\n",
    "from os.path import expanduser, join\n",
    "from subprocess import call\n",
    "import random\n",
    "\n",
    "# From pytorch-playground example\n",
    "model_raw, ds_fetcher, is_imagenet = selector.select('mnist')\n",
    "model_raw.eval()\n",
    "\n",
    "ds_val = ds_fetcher(batch_size=10, train=False, val=True)\n",
    "\n",
    "act = np.empty([0,1], dtype='int64')\n",
    "pred = np.empty([0,1], dtype='int64')\n",
    "\n",
    "# From pytorch-playground example, modified\n",
    "for idx, (data, target) in enumerate(ds_val):\n",
    "    data =  Variable(torch.FloatTensor(data)).cuda()\n",
    "    output = model_raw(data)\n",
    "    act = np.append(act, target.numpy())\n",
    "    pred = np.append(pred, output.data.cpu().numpy().argmax(axis=1)) # read to numpy for use in a DF\n",
    "    \n",
    "model_fit = pd.DataFrame(np.column_stack([act, pred]), columns = ['act','pred'])\n",
    "model_fit['correct'] = model_fit.act==model_fit.pred\n",
    "model_fit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall model accuracy: 98.42%\n",
      "\n",
      "Model accuracy by number:\n",
      "0    : 99.08%\n",
      "1    : 99.21%\n",
      "2    : 98.55%\n",
      "3    : 98.22%\n",
      "4    : 98.57%\n",
      "5    : 98.09%\n",
      "6    : 98.75%\n",
      "7    : 97.76%\n",
      "8    : 97.64%\n",
      "9    : 98.22%\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Summary stats\n",
    "print(\"Overall model accuracy: {:.2f}%\".format(100*model_fit.correct.mean()))\n",
    "print(\"\\nModel accuracy by number:\")\n",
    "print(model_fit.groupby('act', as_index=False).correct.mean().apply(lambda row: \": {:.2f}%\".format(100*row['correct']), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than create my own dataset of handwritten digits, I'm going to use the handwritten portion of the [Char74K](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/) dataset.\n",
    "\n",
    "The below will extract it into your current folder.\n",
    "\n",
    "The images are 1-indexed, 0-9; so while the filenames will still be n+1, this will put the images in the correct class folder.\n",
    "\n",
    "Additionally, this block will select a random 10 images from each class, in keeping with Papernot's methodology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "# get the data\n",
    "curr = os.getcwd()\n",
    "home = expanduser(\"~\")\n",
    "dl_loc = join(home,\"downloads/char76k\")\n",
    "\n",
    "call([\"mkdir\",\"-p\",dl_loc])\n",
    "os.chdir(dl_loc)\n",
    "\n",
    "call([\"wget\",\"http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishHnd.tgz\"])\n",
    "call([\"tar\",\"-xvzf\", \"EnglishHnd.tgz\"])\n",
    "\n",
    "os.chdir(curr)\n",
    "for i in range(10):\n",
    "    new_folder = \"./data/{}\".format(i)\n",
    "    call([\"mkdir\",\"-p\", new_folder])\n",
    "    digit_folder = join(dl_loc, \"English/Hnd/Img/Sample{:03d}/\".format(i+1))\n",
    "    samples = random.sample(os.listdir(digit_folder),10)\n",
    "    for sample in samples:\n",
    "        call(['cp', join(digit_folder, sample), new_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 900)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1200x900 at 0x7F8E28492D30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file = join('./data/0/', os.listdir('./data/0/')[0])\n",
    "img = Image.open(img_file)\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little big... and the colors are reversed. Let's invert and shrink it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAVCAIAAADaYBkLAAABE0lEQVR4nO2TMWqFQBCGZ1ZBQUW0s3xgIegprLyVNl7AxhOIF7D1EFqJtRYKVhaCsE6KJY8QQt6LT0iR/NXPMvPNsv8swL9+RYh4MU6SJOHv5hopiiIMY+wlECIiommaZVlO09Q0TRiGr3JlWQaANE2JqOu6YRj2fQ+CABHPc0U4fd+3bavruuM4+74XRXGfd5JoGMZxHFmWicO6rsdxfKb965mISESapiHiPM8i92VZVFX9WPAzqGhY15Vz7rou5xwAPM+bpukh8TuJ21VVtW3b7XaLooiI4jiG028KAIwxRPR9f11XIiKipmksyxKrdhIK73EFQZDneZIktm3DJV/200o+SXxcxBgTaM75yXz+mN4A6zto1LjNpeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x21 at 0x7F8E28492DD8>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = ImageOps.invert(img) # Invert black and white\n",
    "img.thumbnail((28, 28), Image.ANTIALIAS) # shrinking down to 28*28\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better... Now let's do the same with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABP0lEQVR4nL3SP0hCURQG8E/v4V188CApGoTeJAgNTU01BYFNtjQEQZORUDhEWy0NQeBqQ0NtQUMUCEGDIBgNbdEWOCk4hKFw4T2enAsNqe9P2tDQXe7w43z38HFjZUw+8V/sP5BoIloms0XjkZRjm4rlOJS8N/NwX895QR3kmE7hNZMBzu6OpReZlM7WUw+NEnbWFzRF0MD7lDhxNgUKeU78eLOq9aNq9ip63nIj6KIkzi9k8g1iv+hvHAcA4mQONwpMRUxvQERiZ23USYA78GmEqos0a1AaUBFkatdwZIH4Fq1L6PBkAiv6oOF11z693Su/hW/UeBbex+FqalG2YYxiY+VBfVjWLyqL/GlLc2ShvrRTYimLYrWBkQ2LZzQ720rPVa4DvQ9jAeLgFZoEmBJ9GG7QfAQrwEPo/P1rfgF0HWt8YZeLjAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F8DB2E0F668>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(img_file)\n",
    "\n",
    "digit_transform = transforms.Compose([\n",
    "            lambda x: ImageOps.invert(x),\n",
    "            transforms.CenterCrop(700), # I was getting squished images. CenterCroping makes it square\n",
    "            transforms.Resize((28,28), Image.ANTIALIAS),\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)), # Technically cheating, but we can assume the victim would do this\n",
    "        ])\n",
    "\n",
    "to_img = transforms.ToPILImage()\n",
    "print(digit_transform(img).shape)\n",
    "img_t = to_img(digit_transform(img))\n",
    "img_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an ImageFolder and Dataset. Leveraging code from [this gist](https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb) for adding in the train/test split for random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_folder = datasets.ImageFolder('./data/', transform=digit_transform)\n",
    "\n",
    "im_loader = torch.utils.data.DataLoader(im_folder, \n",
    "                    batch_size=10,\n",
    "                    num_workers=4,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Box Model 49% accurate on new data\n"
     ]
    }
   ],
   "source": [
    "victim_pred = []\n",
    "correct = 0\n",
    "\n",
    "for batch, (data, target) in enumerate(im_loader):\n",
    "\n",
    "    data, target = Variable(data.cuda(), volatile=True), Variable(target.cuda())\n",
    "    \n",
    "    output = model_raw(data)\n",
    "    pred = output.data.max(1, keepdim=True)[1] # same as argmax\n",
    "    \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    victim_pred.append(pred) # collect predicted class for oracle\n",
    "    \n",
    "print(\"Black Box Model {:0.0f}% accurate on new data\".format(correct/len(im_folder)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49% isn't great, so this dataset may be too different from MNIST digits.\n",
    "\n",
    "*self: Add in the 150 MNIST test images like the paper, and use open-cv to create your own \"hand-written\" images*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 32, kernel_size=5) # I originally tried fewer filters, but these are closer to the paper\n",
    "        self.fc1 = nn.Linear(512, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "oracle = Net()\n",
    "oracle.cuda()\n",
    "\n",
    "optimizer = optim.Adam(oracle.parameters(), lr=0.05) #tune up LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "    0     0     0     0     0     0     0     0     1     0\n",
       "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Forward pass noise to ensure the dimensions are right\n",
    "oracle(Variable(torch.rand([1,1,28,28]).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/100 (0%)]\tLoss: -0.103566\n",
      "Train Epoch: 0 [90/100 (90%)]\tLoss: -0.300000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 1 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 1 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 2 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 2 [90/100 (90%)]\tLoss: -0.200000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 3 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 3 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 4 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 4 [90/100 (90%)]\tLoss: 0.000000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 5 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 6 [0/100 (0%)]\tLoss: -0.400000\n",
      "Train Epoch: 6 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 7 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 7 [90/100 (90%)]\tLoss: -0.200000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 8 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 8 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 17% accurate on black box\n",
      "Train Epoch: 9 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 9 [90/100 (90%)]\tLoss: -0.200000\n",
      "Oracle 17% accurate on black box\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    correct = 0\n",
    "    oracle.train()\n",
    "    for batch, (data, t) in enumerate(im_loader):       \n",
    "        t = t.cuda()\n",
    "        t = Variable(t)\n",
    "        data = data.cuda()\n",
    "        data = Variable(data)\n",
    "        \n",
    "        bb_out = model_raw(data)\n",
    "        target = Variable(bb_out.data.max(1, keepdim=True)[1]).view(10,)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = oracle(data)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # same as argmax\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 9 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(data), len(im_loader.dataset),\n",
    "                100. * batch / len(im_loader), loss.data[0]))\n",
    "            \n",
    "    print(\"Oracle {:0.0f}% accurate on black box\".format(correct/len(im_folder)*100))\n",
    "    \n",
    "for i in range(10):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*self: debug, oracle doesn't seem to be learning*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3dl]",
   "language": "python",
   "name": "conda-env-py3dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
