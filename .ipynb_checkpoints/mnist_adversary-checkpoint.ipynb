{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building and initializing mnist parameters\n",
      "Sequential(\n",
      "  (fc1): Linear(in_features=784, out_features=256)\n",
      "  (relu1): ReLU()\n",
      "  (drop1): Dropout(p=0.2)\n",
      "  (fc2): Linear(in_features=256, out_features=256)\n",
      "  (relu2): ReLU()\n",
      "  (drop2): Dropout(p=0.2)\n",
      "  (out): Linear(in_features=256, out_features=10)\n",
      ")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building MNIST data loader with 1 workers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>act</th>\n",
       "      <th>pred</th>\n",
       "      <th>correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   act  pred  correct\n",
       "0    8     8     True\n",
       "1    9     9     True\n",
       "2    3     3     True\n",
       "3    5     5     True\n",
       "4    3     3     True"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torchvision import datasets, transforms\n",
    "from utee import selector\n",
    "from PIL import Image, ImageOps\n",
    "from os.path import expanduser, join\n",
    "from subprocess import call\n",
    "import random\n",
    "\n",
    "# From pytorch-playground example\n",
    "model_raw, ds_fetcher, is_imagenet = selector.select('mnist')\n",
    "model_raw.eval()\n",
    "\n",
    "ds_val = ds_fetcher(batch_size=10, train=False, val=True)\n",
    "\n",
    "act = np.empty([0,1], dtype='int64')\n",
    "pred = np.empty([0,1], dtype='int64')\n",
    "\n",
    "# From pytorch-playground example, modified\n",
    "for idx, (data, target) in enumerate(ds_val):\n",
    "    data =  Variable(torch.FloatTensor(data)).cuda()\n",
    "    output = model_raw(data)\n",
    "    act = np.append(act, target.numpy())\n",
    "    pred = np.append(pred, output.data.cpu().numpy().argmax(axis=1))\n",
    "    \n",
    "model_fit = pd.DataFrame(np.column_stack([act, pred]), columns = ['act','pred'])\n",
    "model_fit['correct'] = model_fit.act==model_fit.pred\n",
    "model_fit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall model accuracy: 98.42%\n",
      "\n",
      "Model accuracy by number:\n",
      "0    : 99.08%\n",
      "1    : 99.21%\n",
      "2    : 98.55%\n",
      "3    : 98.22%\n",
      "4    : 98.57%\n",
      "5    : 98.09%\n",
      "6    : 98.75%\n",
      "7    : 97.76%\n",
      "8    : 97.64%\n",
      "9    : 98.22%\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Summary stats\n",
    "print(\"Overall model accuracy: {:.2f}%\".format(100*model_fit.correct.mean()))\n",
    "print(\"\\nModel accuracy by number:\")\n",
    "print(model_fit.groupby('act', as_index=False).correct.mean().apply(lambda row: \": {:.2f}%\".format(100*row['correct']), axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than create my own dataset of handwritten digits, I'm going to use the handwritten portion of the [Char74K](http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/) dataset.\n",
    "\n",
    "The below will extract it into your current folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1234)\n",
    "\n",
    "# get the data\n",
    "curr = os.getcwd()\n",
    "home = expanduser(\"~\")\n",
    "dl_loc = join(home,\"downloads/char76k\")\n",
    "\n",
    "call([\"mkdir\",\"-p\",dl_loc])\n",
    "os.chdir(dl_loc)\n",
    "\n",
    "call([\"wget\",\"http://www.ee.surrey.ac.uk/CVSSP/demos/chars74k/EnglishHnd.tgz\"])\n",
    "call([\"tar\",\"-xvzf\", \"EnglishHnd.tgz\"])\n",
    "\n",
    "os.chdir(curr)\n",
    "for i in range(10):\n",
    "    new_folder = \"./data/{}\".format(i)\n",
    "    call([\"mkdir\",\"-p\", new_folder])\n",
    "    digit_folder = join(dl_loc, \"English/Hnd/Img/Sample{:03d}/\".format(i+1))\n",
    "    samples = random.sample(os.listdir(digit_folder),10)\n",
    "    for sample in samples:\n",
    "        call(['cp', join(digit_folder, sample), new_folder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1200, 900)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=1200x900 at 0x7F889835F828>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_file = join('./data/0/', os.listdir('./data/0/')[0])\n",
    "img = Image.open(img_file)\n",
    "print(img.size)\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Little big... and the colors are reversed. Let's invert and shrink it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAVCAIAAADaYBkLAAABE0lEQVR4nO2TMWqFQBCGZ1ZBQUW0s3xgIegprLyVNl7AxhOIF7D1EFqJtRYKVhaCsE6KJY8QQt6LT0iR/NXPMvPNsv8swL9+RYh4MU6SJOHv5hopiiIMY+wlECIiommaZVlO09Q0TRiGr3JlWQaANE2JqOu6YRj2fQ+CABHPc0U4fd+3bavruuM4+74XRXGfd5JoGMZxHFmWicO6rsdxfKb965mISESapiHiPM8i92VZVFX9WPAzqGhY15Vz7rou5xwAPM+bpukh8TuJ21VVtW3b7XaLooiI4jiG028KAIwxRPR9f11XIiKipmksyxKrdhIK73EFQZDneZIktm3DJV/200o+SXxcxBgTaM75yXz+mN4A6zto1LjNpeQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=28x21 at 0x7F88987E7A90>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = ImageOps.invert(img) # Invert black and white\n",
    "img.thumbnail((28, 28), Image.ANTIALIAS) # shrnking down to 28*28\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks much better... Now let's do the same with PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABB0lEQVR4nGOcwoAbMOGRG3ySLAgmr92H89+wS2pbrol7qXWo7Q+SJCM0EHhT1soyMPxNkfFBkoXZWZ0py7Cn7uf86ylYHLQp5Gei8daXPxeps2NIsq/nPalwd1JiPxeDOYakbD3Dyo6fDP0GDNmxGJIKjgxbbzEwfCv9q+KIISkixXD+PQMDwzMGBkw7v31gUORlYGDQZf77GUPy7iGGRBMGBsGZDJc2YUjeCfmbf4iFoeUQQ8VihCQ0+H7+OWmVInltw6/PnNMwAyHyxbeZ+v7fGKyeIwUfLGwZIq79FWD47K6QzYCpk2GFmC0nm/tLZDmETgYGFmGGt8gRhhLZf14yoAEaJTAAsk9QVHgf8ooAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=28x28 at 0x7F88987D8A90>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open(img_file)\n",
    "\n",
    "digit_transform = transforms.Compose([\n",
    "            lambda x: ImageOps.invert(x),\n",
    "            transforms.CenterCrop(700), # I was getting squished images. CenterCroping makes it square\n",
    "            transforms.Resize((28,28)),\n",
    "            transforms.Grayscale(1),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.1307,), (0.3081,)),\n",
    "        ])\n",
    "\n",
    "to_img = transforms.ToPILImage()\n",
    "print(digit_transform(img).shape)\n",
    "img_t = to_img(digit_transform(img))\n",
    "img_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up an ImageFolder and Dataset. Leveraging code from [this gist](https://gist.github.com/kevinzakka/d33bf8d6c7f06a9d8c76d97a7879f5cb) for adding in the train/test split for random sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_folder = datasets.ImageFolder('./data/', transform=digit_transform)\n",
    "\n",
    "im_loader = torch.utils.data.DataLoader(im_folder, \n",
    "                    batch_size=10,\n",
    "                    num_workers=4,\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black Box Model 50% accurate on new data\n"
     ]
    }
   ],
   "source": [
    "victim_pred = []\n",
    "correct = 0\n",
    "\n",
    "for batch, (data, target) in enumerate(im_loader):\n",
    "\n",
    "    data, target = Variable(data.cuda(), volatile=True), Variable(target.cuda())\n",
    "    \n",
    "    output = model_raw(data)\n",
    "    pred = output.data.max(1, keepdim=True)[1] # same as argmax\n",
    "    \n",
    "    correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "    \n",
    "    victim_pred.append(pred) # collect predicted class for oracle\n",
    "    \n",
    "print(\"Black Box Model {:0.0f}% accurate on new data\".format(correct/len(im_folder)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 32, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(512, 20)\n",
    "        self.fc2 = nn.Linear(20, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n",
    "        x = x.view(-1, 512)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=1)\n",
    "    \n",
    "oracle = Net()\n",
    "oracle.cuda()\n",
    "\n",
    "optimizer = optim.Adam(oracle.parameters(), lr=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.1085  0.1104  0.0922  0.0949  0.1093  0.0838  0.0905  0.0921  0.1002  0.1180\n",
       "[torch.cuda.FloatTensor of size 1x10 (GPU 0)]"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oracle(Variable(torch.rand([1,1,28,28]).cuda()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/100 (0%)]\tLoss: -0.096423\n",
      "Train Epoch: 0 [90/100 (90%)]\tLoss: -0.300000\n",
      "Oracle 11% accurate on black box\n",
      "Train Epoch: 1 [0/100 (0%)]\tLoss: -0.300000\n",
      "Train Epoch: 1 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 2 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 2 [90/100 (90%)]\tLoss: 0.000000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 3 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 3 [90/100 (90%)]\tLoss: -0.200000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 4 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 4 [90/100 (90%)]\tLoss: -0.200000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 5 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 5 [90/100 (90%)]\tLoss: -0.300000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 6 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 6 [90/100 (90%)]\tLoss: 0.000000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 7 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 7 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 8 [0/100 (0%)]\tLoss: -0.200000\n",
      "Train Epoch: 8 [90/100 (90%)]\tLoss: 0.000000\n",
      "Oracle 12% accurate on black box\n",
      "Train Epoch: 9 [0/100 (0%)]\tLoss: -0.100000\n",
      "Train Epoch: 9 [90/100 (90%)]\tLoss: -0.100000\n",
      "Oracle 12% accurate on black box\n"
     ]
    }
   ],
   "source": [
    "def train(epoch):\n",
    "    correct = 0\n",
    "    oracle.train()\n",
    "    for batch, (data, t) in enumerate(im_loader):       \n",
    "        t = t.cuda()\n",
    "        t = Variable(t)\n",
    "        data = data.cuda()\n",
    "        data = Variable(data)\n",
    "        \n",
    "        bb_out = model_raw(data)\n",
    "        target = Variable(bb_out.data.max(1, keepdim=True)[1]).view(10,)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = oracle(data)\n",
    "        \n",
    "        pred = output.data.max(1, keepdim=True)[1] # same as argmax\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch % 9 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch * len(data), len(im_loader.dataset),\n",
    "                100. * batch / len(im_loader), loss.data[0]))\n",
    "            \n",
    "    print(\"Oracle {:0.0f}% accurate on black box\".format(correct/len(im_folder)*100))\n",
    "    \n",
    "for i in range(10):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3dl]",
   "language": "python",
   "name": "conda-env-py3dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
